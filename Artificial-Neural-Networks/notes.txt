Components in ANN:

Neuron:

Body of the Neuron
Dendrites
Axon

Neuron on it's own is not that strong. Lots of neuron work together to create magic

Dendrites receive signal. Connected to axon of other neuron
Axon transmit signal. Connected to dendrite of another neuron

Synapses - Where the signal is passed between two neurons

Neuron gets input signal and has an output signal 
Input signals are represented by other neurons 

Input layer - Is senses in the human body. Feel, smell, see, hear.

Input layer passes signal/data through synpase to the hidden layer neuron. Input values are independent variable1. These input values are for one row say in a database. These variables need to be standardised or be normalised

Output value can be continous (price), binary (will exit yes/no) or categorical. Output value will also be for the same observation

Synapses are assigned weights. Weights are crucial for functioning. Weights are how NNs learn by adjusting weights to understand what signal is important or not. E.g. Gradient Descent and Backpropogation

Inside the hidden neuron layer - A weighted sum of the input values * weight is done. An activation function is applied. From that the neuron understands if a signal is to be passed or not. 


The Activation Function:

4 different types of activation function (predominant ones): a) Threshold function: Simple function. If value < 0 then the threshold function passes 0 else passes 1
b) Sigmoid Function: There's a formula. 1/(1+e^-x). Used in logistic regression. Much smoother than threshold function. Gradual progression. Used for predicting probabilities. Also between 0 and 1
c) Rectifier Function: Popular for ANN. Goes from 0 to 1. 
d) Hyperbolic Tangent (tanh): Similar to sigmoid. It goes below from 0 to -1. There's a formula for the hyperbolic tangent. 


Combination - Rectifier used in Hidden Layer and Sigmoid used at the output layer for predicting probability 

How do Neural Networks Work:

Taking an example for House price prediction

Parameters (Input): Area, Bedrooms, Distance to City, Age act as the input layer 

Output Layer (Price): Price that is being predicted
Price would look like = w1*x1 + w2*x2 + ... +wm*xm

Increase in accuracy comes from the hidden layer. 

Some weights might have zero values since input may not be important. First neuron will concentrate on Area and Distance to city
Another neuron will concentrate on Area, Bedrooms and Age Parameters. Three parameters are converted to a brand new parameter to be more precise.
Anothe neuron might have picked up just one parameter that is age. 
Another neuron might have picked up Bedroom and distance parameter
Another might have picked up combination of all parameters

How do Neural Networks learn :

Goal is to create a network which learns on it own. 

Perceptron was invented by Frank. Something that learns and adjusts itself
Cost function tells what the error is in the prediction. The lower the cost function the better
A perceptron/Neural Network learns by adjusting weights of the input vals once the cost function is implemented to reduce the cost function

One Epoch is when we go through a whole dataset and train the neural network on all the rows. All the rows share the same weight during an Epoch
To adjust weights again, we go back again. Process is called Backpropogation

Gradient Descent - 
 A chart of cost function values is taken. It looks like a descent. The best would be the one with the least value
 Curse of Dimensionality - We cannot brute force weights since it will take a long time and costs.

A point is taken on the curve. Slope is drawn. Check if the slope is downward or upward. Points are taken until there is no more downward or upward slope. Also called Descent because you are looking for the lower cost function

Stochastic Gradient Descent:

This method doesn't requires the cost function to be convex(curve) like the gradient descent i.e. it has one global minimum. 

The rows are taken one by one and the weights are adjusted. Rows are taken one after another. Gradient descent is batch wise and Stochastic is row wise
Stochastic helps in avoiding local minimums instead of global minimum

Backpropogation:

Advanced algorithm. which allows to adjust weights backwards simultaneously

ANN Steps with Stochastic Gradient descent

STEP1 : Randomly initialise weights to small numbers close to 0 
STEP2: Input first observation of your dataset in the input layer, each feature in one input code 
STEP3: Forward propogation - Neurons activated in a way that impact of each neurons activation is limited by the weights. Propagate till activations until getting predicted result y
STEP4: Compare predicted result to actual result. Measure generated error 
STEP5: Back propagation: from right to left. Error back propagated. Update weights according to how much they are responsible for the rror. Learning rate decides how much we should update weights
STEP6: Repeat 1 to 5 and update weights after each observation. Or repeat step1 to 5 but update weights only after a batch of observations
