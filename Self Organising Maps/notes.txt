Self Organising Maps

Self Organising Maps are used for feature detection and fall under Unsupervised Version

Teuvo Kohonen - Person who created SOM

Used to reduce dimensions

A multi dimensional dataset is taken and the dimensionality is reduced. A map is produced. Used to reduce the amount of columns

K-Means which does clustering would aid in Self Organising Maps 

K-Means clustering - 
Dataset plotted would undergo the K-Means algorithm giving us clustered 

Step1 - The number of clusters 'k' is chosen 
Step2 - Select at random K point, which are the centroids from the cluster 
Step3 - Assign each data point to the closest centroid that forms K clusters 
Step4 - Compute and place the new centroid of each cluster 
Step5 - Reassign each data point to the closest centroid. if any reassignent took place, go to step 4 otherwise finish

For assigning data point - euclidean distance is something that can be used 

How do SOMs learn- 
Weights in SOMs are different than the weights in ANN. 
In SOMs no activation functions. Weights are a characteristic of the node itself. 

A comparison is done with the node and the dataset. The closest one to the original input space is found out. 
To find out the euclidean distance is taken with the input and the node. The least distance is the closest
SOMs will update the weights for the best unit so that it is even more closer to the input meaning the SOM is coming closer to that data point
The radius from the closest point will have all the corresponding weights updated.
The next one is calculated and the steps are created and then clusters are created by seeing which other weight is closer and corresponsding color is added
Best Matching Unit - BMU
Each BMU is calculated and the radius around them is taken
In the next iteration after assigning all the closest colors, the radius sinks 
The radius keeps sinking

SOm Features
SOMs retain topology of the input set 
SOMs reveal correlations that are not easily identified
SOMs classify data without supervision
No target vector -> No backpropagation
No lateral connections between output nodes

K Means - Random Initialisation Trap
Taking an example
a) Trying to build 3 clusters 
b) Random K points, the centroids are selected
c) Step 4 and 5 from creating the clusters 

The actual and the one produced are different.
The solution to avoid these would be to use the K means ++ algorithm

K Means Intuition - Choosing the right number of clusters 

Choosing the right number of clusters 

WCSS = sum for each cluster and distance calculated
If there are as many clusters as data points, WCSS would equate to 0 
Elbow method used to determine the right number of clusters. The drop indicates the optimal number of clusters 
